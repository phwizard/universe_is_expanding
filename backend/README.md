# ğŸ§  DeepView Backend - Semantic Continuum Explorer

**DeepView Backend** is a FastAPI-based service that powers the Semantic Continuum Explorer, providing intelligent idea generation and semantic search capabilities using TinyLlama language model and FAISS vector indexing.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP Requests    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   FastAPI        â”‚
â”‚   (React 3D)    â”‚                     â”‚   Backend        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Core Components                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   TinyLlama     â”‚  â”‚   FAISS Index   â”‚  â”‚   Embedding â”‚  â”‚
â”‚  â”‚   1.1B Chat     â”‚  â”‚   (In-Memory)   â”‚  â”‚   Function  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

### ğŸ¤– AI-Powered Idea Generation
- **TinyLlama 1.1B Chat Model**: Lightweight but powerful language model for generating diverse ideas
- **Creative Expansion**: Generates 15-25 related ideas from any input sentence
- **Diverse Output**: Mix of factual, hypothetical, creative, and narrative continuations

### ğŸ” Semantic Search & Storage
- **FAISS Vector Index**: High-performance similarity search using Facebook AI Similarity Search
- **In-Memory Storage**: Fast access to embedded sentences and their relationships
- **Nearest Neighbor Search**: Find semantically similar ideas in the knowledge space

### ğŸŒ RESTful API
- **FastAPI Framework**: Modern, fast web framework with automatic OpenAPI documentation
- **CORS Support**: Configured for frontend integration
- **Type Safety**: Pydantic models for request/response validation

## ğŸ“¡ API Endpoints

### 1. `/expand_continuum` (POST)
**Purpose**: Generate diverse related ideas from an input sentence

**Request**:
```json
{
  "sentence": "The universe is expanding"
}
```

**Response**:
```json
{
  "nodes": [
    "Dark energy may be accelerating this process",
    "Could the universe collapse back into a singularity?",
    "What if each galaxy is a neuron?",
    "The expansion rate is measured by Hubble's constant",
    "Multiverse theory suggests parallel universes"
  ]
}
```

**Implementation Details**:
- Uses creative prompting with high temperature (1.0) for diversity
- Generates up to 150 tokens with top-k and top-p sampling
- Parses bullet points and falls back to sentence splitting

### 2. `/expand` (POST)
**Purpose**: Legacy endpoint for simpler idea expansion

**Request**:
```json
{
  "sentence": "Artificial intelligence"
}
```

**Response**:
```json
{
  "ideas": [
    "Machine learning algorithms",
    "Neural networks and deep learning",
    "Natural language processing"
  ]
}
```

### 3. `/embed` (POST)
**Purpose**: Store a sentence in the semantic index

**Request**:
```json
{
  "sentence": "Quantum computing uses qubits",
  "neighbors": 5
}
```

**Response**:
```json
{
  "message": "Sentence embedded and added."
}
```

### 4. `/search` (POST)
**Purpose**: Find semantically similar sentences

**Request**:
```json
{
  "sentence": "Quantum mechanics",
  "neighbors": 5
}
```

**Response**:
```json
{
  "neighbors": [
    "Quantum computing uses qubits",
    "Wave-particle duality",
    "Heisenberg uncertainty principle"
  ]
}
```

## ğŸ”§ Technical Implementation

### Model Loading
```python
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()
```

### Embedding Generation
```python
@torch.no_grad()
def get_embedding(text: str):
    tokens = tokenizer(text, return_tensors="pt", truncation=True)
    outputs = model(**tokens, output_hidden_states=True)
    hidden = outputs.hidden_states[-1].squeeze(0)
    return hidden.mean(dim=0).cpu().numpy()
```

### FAISS Index Setup
```python
DIM = model.config.hidden_size  # Typically 768 for TinyLlama
index = faiss.IndexFlatL2(DIM)  # L2 distance for similarity
sentence_store = []  # In-memory sentence storage
```

## ğŸ› ï¸ Setup & Installation

### Prerequisites
- Python 3.10+
- CUDA-compatible GPU (recommended for faster inference)
- 4GB+ RAM for model loading

### Installation
```bash
# Clone the repository
git clone <repository-url>
cd deepview/backend

# Install dependencies
pip install fastapi uvicorn transformers torch faiss-cpu pydantic

# For GPU support (optional)
pip install faiss-gpu

# Run the server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Environment Variables
```bash
# Optional: Set for production
export MODEL_CACHE_DIR="/path/to/model/cache"
export CUDA_VISIBLE_DEVICES="0"
```

## ğŸ“Š Performance Characteristics

### Model Specifications
- **Model**: TinyLlama-1.1B-Chat-v1.0
- **Parameters**: 1.1 billion
- **Embedding Dimension**: 768
- **Memory Usage**: ~2GB RAM
- **Inference Speed**: ~50-100ms per request (CPU), ~10-20ms (GPU)

### API Performance
- **Response Time**: 100-500ms for idea generation
- **Concurrent Requests**: Supports multiple simultaneous users
- **Memory**: In-memory FAISS index scales with stored sentences

## ğŸ”’ Security & Configuration

### CORS Configuration
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Production Considerations
- Use `allow_origins=["*"]` only for development
- Implement rate limiting for production
- Add authentication for sensitive endpoints
- Consider using Redis for persistent storage

## ğŸ§ª Testing

### Manual Testing
```bash
# Test idea expansion
curl -X POST "http://localhost:8000/expand_continuum" \
     -H "Content-Type: application/json" \
     -d '{"sentence": "Climate change"}'

# Test semantic search
curl -X POST "http://localhost:8000/search" \
     -H "Content-Type: application/json" \
     -d '{"sentence": "Global warming", "neighbors": 3}'
```

### API Documentation
Visit `http://localhost:8000/docs` for interactive API documentation (Swagger UI)

## ğŸ”® Future Enhancements

### Planned Features
- **Persistent Storage**: Database integration for session persistence
- **RAG Integration**: Connect ideas to real documents and sources
- **Multi-Model Support**: Switch between different LLMs
- **Advanced Vector Search**: Implement semantic clustering and visualization
- **User Sessions**: Track exploration paths and favorites

### Technical Improvements
- **Model Optimization**: Quantization for faster inference
- **Caching**: Redis-based response caching
- **Load Balancing**: Horizontal scaling support
- **Monitoring**: Prometheus metrics and health checks

## ğŸ“ License

(c) DeepX, Taras Filatov, 2025

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ Support

For questions or issues, please open an issue in the repository or contact the development team. 